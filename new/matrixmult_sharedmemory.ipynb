{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Matrix Multiplication with Shared Memory\n",
    "\n",
    "\n",
    "This is a sequel to the [CUDA Matrix Multiplication][prevous_example] example.\n",
    "\n",
    "This document introduces the CUDA shared memory.  We will demonstrate its\n",
    "usage for accelerating the matrix multiplication code developed preivously.\n",
    "\n",
    "\n",
    "[prevous_example]: https://github.com/ContinuumIO/numbapro-examples/blob/renew/new/matrixmult_intro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division\n",
    "\n",
    "# builtin packages\n",
    "import sys\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "# extra packages\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba import cuda, float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version information:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file is generated on: 2015-06-05 11:36:15.642841\n",
      "python: 3.4\n",
      "numpy: 1.9.2\n",
      "numba: 0.19.1\n",
      "CUDA GPU: b'GeForce GT 650M'\n"
     ]
    }
   ],
   "source": [
    "print(\"This file is generated on:\", datetime.datetime.now())\n",
    "print(\"python: {0}.{1}\".format(*sys.version_info[:2]))\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"numba:\", numba.__version__)\n",
    "print(\"CUDA GPU:\", cuda.gpus[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Naive GPU Version\n",
    "\n",
    "We will reuse the matrix multiplication code from the previous example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def gpu_matrix_mult(matA, matB, matC):\n",
    "    # Read special register for thread ID\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    bw = cuda.blockDim.x\n",
    "    bh = cuda.blockDim.y\n",
    "\n",
    "    # Get global thread ID\n",
    "    x = tx + bx * bw\n",
    "    y = ty + by * bh\n",
    "\n",
    "    # Get bounds\n",
    "    m, n = matC.shape\n",
    "    k = matB.shape[0]\n",
    "\n",
    "    # Check for out-of-bound\n",
    "    if x >= m or y >= n:\n",
    "        # This is an extra thread.  Exit.\n",
    "        return\n",
    "\n",
    "    # The actual computation per output element\n",
    "    res = 0\n",
    "    for i in range(k):\n",
    "        res += matA[x, i] * matB[i, y]\n",
    "    # Store the result\n",
    "    matC[x, y] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize with Shared Memory and Block Algorithm\n",
    "\n",
    "A faster way to implement the matrix multiplication is to use a block\n",
    "algorithm. The CUDA thread hierarchy fits naturally to this.  We can map CUDA\n",
    "blocks to matrix blocks and map CUDA threads to elements in each block.\n",
    "\n",
    "### How is this faster?\n",
    "\n",
    "To understand, we need to understand a little bit about the CUDA **memory\n",
    "hierarchy**. Unlike CPU, CUDA exposes a *shared memory* unit that is private to\n",
    "each block and acts like a manual cache.  Data transfer from the shared\n",
    "memory is a lot faster than from the *global memory*, which is accessible from\n",
    "the GPU and CPU.  The ``cuda.to_device()`` puts data to the *global memory*.\n",
    "\n",
    "The block algorithm loads each matrix block into the shared memory before\n",
    "computing the product for the block.  This allows all the threads computing\n",
    "on the current matrix block to reuse memory loaded into the shared memory\n",
    "(our manual cache) instead of from the slower global memory.\n",
    "\n",
    "### Synchronization\n",
    "\n",
    "In each CUDA block, threads are cooperatively loading data into shared\n",
    "memory.  These threads are running concurrently and they may not be executing\n",
    "the same instruction.  We need a barrier ``cuda.syncthreads()`` to ensure\n",
    "all the threads have executed up to a certain point.  We need one before the\n",
    "data preload to sure the shared memory is not modified while some threads are\n",
    "still using it.  We need one after the data preload to ensure all the threads\n",
    "have completed the preloading.\n",
    "\n",
    "**Important Note**\n",
    "\n",
    "The barrier ``cuda.syncthreads()`` blocks **all** threads in the current block\n",
    "until all of them have reached the same location.  The behavior is\n",
    "**undefined** if some threads have returned.  Therefore, we need to keep all\n",
    "the threads in the block alive if some of them may execute a barrier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block_per_grid = 10\n",
    "thread_per_block = 16\n",
    "\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_blocked_matrix_mult(matA, matB, matC):\n",
    "    # Define shared array\n",
    "    smA = cuda.shared.array(shape=(thread_per_block, thread_per_block),\n",
    "                            dtype=float32)\n",
    "    smB = cuda.shared.array(shape=(thread_per_block, thread_per_block),\n",
    "                            dtype=float32)\n",
    "\n",
    "    # Get thread IDs\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    bw = cuda.blockDim.x\n",
    "    bh = cuda.blockDim.y\n",
    "\n",
    "    # Get global ID\n",
    "    x = tx + bx * bw\n",
    "    y = ty + by * bh\n",
    "\n",
    "    # Get bounds\n",
    "    m, n = matC.shape\n",
    "\n",
    "    # Bound check\n",
    "    in_bound = x < m and y < n\n",
    "\n",
    "    # Computation starts here\n",
    "    acc = 0\n",
    "\n",
    "    # For each block\n",
    "    for i in range(block_per_grid):\n",
    "        # Wait for all threads to reach this point\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        if in_bound:\n",
    "            # Cooperatively load from global memory into faster shared memory\n",
    "            smA[tx, ty] = matA[x, ty + i * thread_per_block]\n",
    "            smB[tx, ty] = matB[tx + i * thread_per_block, y]\n",
    "\n",
    "        # Wait for all threads to reach this point\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        if in_bound:\n",
    "            # Compute using data in shared memory\n",
    "            for j in range(thread_per_block):\n",
    "                acc += smA[tx, j] * smB[j, ty]\n",
    "\n",
    "    if in_bound:\n",
    "        # Store result\n",
    "        matC[x, y] = acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_dim_large = [block_per_grid * thread_per_block] * 2\n",
    "\n",
    "matA = np.random.random(mat_dim_large).astype(np.float32)\n",
    "matB = np.random.random(mat_dim_large).astype(np.float32)\n",
    "\n",
    "gpu_result = np.zeros_like(matA)\n",
    "\n",
    "griddim = block_per_grid, block_per_grid\n",
    "blockdim = thread_per_block, thread_per_block\n",
    "\n",
    "gpu_blocked_matrix_mult[griddim, blockdim](matA, matB, gpu_result)\n",
    "\n",
    "npy_result = np.dot(matA, matB)\n",
    "\n",
    "assert np.allclose(npy_result, gpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for timing function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def time_took(functor):\n",
    "    ts = timer()\n",
    "    functor()\n",
    "    te = timer()\n",
    "    return te - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 naive version: 0.2018 seconds\n",
      "  blocked+sharedmemory version: 0.0065 seconds\n",
      "Speedup: 31.3x\n"
     ]
    }
   ],
   "source": [
    "res_naive = np.empty_like(matA)\n",
    "res_blocked = np.empty_like(matA)\n",
    "\n",
    "gpu1_time = time_took(\n",
    "    lambda: gpu_matrix_mult[griddim, blockdim](matA, matB, res_naive)\n",
    ")\n",
    "gpu2_time = time_took(\n",
    "    lambda: gpu_blocked_matrix_mult[griddim, blockdim](matA, matB, res_blocked)\n",
    ")\n",
    "\n",
    "# result matches?\n",
    "assert np.allclose(res_naive, res_blocked)\n",
    "# faster?\n",
    "assert gpu2_time < gpu1_time\n",
    "\n",
    "fmt = \"{0:>30s}: {1:.4f} seconds\"\n",
    "print(fmt.format(\"naive version\", gpu1_time))\n",
    "print(fmt.format(\"blocked+sharedmemory version\", gpu2_time))\n",
    "print(\"Speedup: {0:.1f}x\".format(gpu1_time / gpu2_time))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}