{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CUDA Matrix Multiplication\n",
    "\n",
    "This document introduces CUDA programming with a simple GPU square matrix\n",
    "multiplication.  The implementation used here is for demonstrating the CUDA\n",
    "parallel programming and the code is not optimized for high performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, print_function, division\n",
    "\n",
    "# builtin packages\n",
    "import sys\n",
    "import datetime\n",
    "from timeit import default_timer as timer\n",
    "# extra packages\n",
    "import numpy as np\n",
    "import numba\n",
    "from numba import cuda, jit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Version information:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This file is generated on: 2015-06-05 10:54:21.276646\n",
      "python: 3.4\n",
      "numpy: 1.9.2\n",
      "numba: 0.19.1\n",
      "CUDA GPU: b'GeForce GT 650M'\n"
     ]
    }
   ],
   "source": [
    "print(\"This file is generated on:\", datetime.datetime.now())\n",
    "print(\"python: {0}.{1}\".format(*sys.version_info[:2]))\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"numba:\", numba.__version__)\n",
    "print(\"CUDA GPU:\", cuda.gpus[0].name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CPU Version\n",
    "\n",
    "This implements a square matrix multiplication using a naive algorithm.  We\n",
    "compile it with Numba for speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@jit\n",
    "def cpu_matrix_mult(matA, matB, matC):\n",
    "    m, n = matC.shape\n",
    "    k = matB.shape[0]\n",
    "    for x in range(m):\n",
    "        for y in range(n):\n",
    "            matC[x, y] = 0\n",
    "            for i in range(k):\n",
    "                matC[x, y] += matA[x, i] * matB[i, y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the CPU Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create small matrices for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_dim_small = 4, 4\n",
    "matA_small = np.random.random(mat_dim_small).astype(np.float32)\n",
    "matB_small = np.random.random(mat_dim_small).astype(np.float32)\n",
    "cpu_result = np.zeros_like(matA_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cpu_matrix_mult(matA_small, matB_small, cpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU result\n",
      "[[ 1.07330048  1.00605381  1.075804    0.66050881]\n",
      " [ 1.02138257  1.02917802  1.1562916   0.76896501]\n",
      " [ 1.37135708  1.24724054  1.36159396  0.97620076]\n",
      " [ 0.87850666  0.65593308  0.90441626  0.47408649]]\n"
     ]
    }
   ],
   "source": [
    "print(\"CPU result\")\n",
    "print(cpu_result)\n",
    "assert np.allclose(np.dot(matA_small, matB_small), cpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A CUDA GPU Version\n",
    "\n",
    "This implements a CUDA GPU version of the matrix multiply.\n",
    "We are using ``@cuda.jit`` to decorate the implementation to compile it into\n",
    "a *CUDA kernel*.  When the kernel function is launched, every thread\n",
    "will execute the same code.  To tell which thread the execution is in,\n",
    "CUDA provides a set of special registers that are accessible with\n",
    "``cuda.threadIdx``, ``cuda.blockIdx`` and ``cuda.blockDim``.  These registers\n",
    "are 2D or 3D vectors of the thread ID, block ID and block dimension,\n",
    "representively.\n",
    "\n",
    "CUDA defines a **thread hierarchy**.  A kernel launch creates a **grid** of\n",
    "**blocks**.  Each block contains **threads**.\n",
    "\n",
    "A common pattern is to compute the global thread ID, as oppose to\n",
    "using the nested thread and block IDs.  In this example, the kernel is launched\n",
    "with a 2D grid and 2D block that the combined dimension matches the shape of\n",
    "the matrices.  Therefore, the flattened global thread ID maps directly to the\n",
    "indices of each element in the matrix.\n",
    "\n",
    "For cases where the matrix shape is not multiple of the block dimension,\n",
    "a common practice is to launch more threads than there are elements.  The extra\n",
    "thread will have nothing to do.  It is important to check for these threads\n",
    "to avoid invalid memory reads and writes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def gpu_matrix_mult(matA, matB, matC):\n",
    "    # Read special register for thread ID\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bx = cuda.blockIdx.x\n",
    "    by = cuda.blockIdx.y\n",
    "    bw = cuda.blockDim.x\n",
    "    bh = cuda.blockDim.y\n",
    "\n",
    "    # Get global thread ID\n",
    "    x = tx + bx * bw\n",
    "    y = ty + by * bh\n",
    "\n",
    "    # Get bounds\n",
    "    m, n = matC.shape\n",
    "    k = matB.shape[0]\n",
    "\n",
    "    # Check for out-of-bound\n",
    "    if x >= m or y >= n:\n",
    "        # This is an extra thread.  Exit.\n",
    "        return\n",
    "\n",
    "    # The actual computation per output element\n",
    "    res = 0\n",
    "    for i in range(k):\n",
    "        res += matA[x, i] * matB[i, y]\n",
    "    # Store the result\n",
    "    matC[x, y] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the CUDA Code\n",
    "\n",
    "Decide of CUDA grid/block dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "block_per_grid = 60\n",
    "thread_per_block = 16\n",
    "\n",
    "griddim = block_per_grid, block_per_grid\n",
    "blockdim = thread_per_block, thread_per_block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create matrices using base on the grid/block dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mat_dim_large = [block_per_grid * thread_per_block] * 2\n",
    "\n",
    "matA = np.random.random(mat_dim_large).astype(np.float32)\n",
    "matB = np.random.random(mat_dim_large).astype(np.float32)\n",
    "\n",
    "gpu_result = np.zeros_like(matA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch kernel\n",
    "\n",
    "The square bracket ``[]`` is overloaded to configure the launch for the grid\n",
    "and block dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpu_matrix_mult[griddim, blockdim](matA, matB, gpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm 0.0236053\n"
     ]
    }
   ],
   "source": [
    "npy_result = np.dot(matA, matB)\n",
    "assert np.allclose(npy_result, gpu_result)\n",
    "print(\"L1 norm\", np.linalg.norm(gpu_result - npy_result, ord=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Memory Transfers\n",
    "\n",
    "By default, numba automatically transfer numpy array memory between\n",
    "the CPU and GPU.  This is convenient but may lead to redundant memory\n",
    "transfers.  Numba will always transfer numpy array back to the CPU.\n",
    "User can control the memory transfer explicit to optimize the process.\n",
    "\n",
    "To copy to the GPU device from the CPU host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numba.cuda.cudadrv.devicearray.DeviceNDArray object at 0x10fd08c18>\n",
      "<numba.cuda.cudadrv.devicearray.DeviceNDArray object at 0x10fd08e10>\n"
     ]
    }
   ],
   "source": [
    "device_matA = cuda.to_device(matA)\n",
    "device_matB = cuda.to_device(matB)\n",
    "print(device_matA)\n",
    "print(device_matB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allocate GPU memory directly.  (It is similar to ``numpy.empty_like``.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<numba.cuda.cudadrv.devicearray.DeviceNDArray object at 0x10f92f320>\n"
     ]
    }
   ],
   "source": [
    "device_matC = cuda.device_array_like(matA)\n",
    "print(device_matC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gpu_matrix_mult[griddim, blockdim](device_matA, device_matB, device_matC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy GPU device memory back to CPU host"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 242.71121216  246.92608643  248.31898499 ...,  245.22866821  247.7897644\n",
      "   237.0383606 ]\n",
      " [ 239.02482605  248.77922058  243.63262939 ...,  250.29560852\n",
      "   250.80892944  245.33778381]\n",
      " [ 240.47009277  246.98405457  241.13589478 ...,  244.33912659\n",
      "   244.43829346  240.039505  ]\n",
      " ..., \n",
      " [ 243.38659668  251.18955994  246.80578613 ...,  246.63110352  241.8449707\n",
      "   239.55197144]\n",
      " [ 244.58570862  247.32484436  247.43658447 ...,  248.44715881\n",
      "   252.43330383  248.55296326]\n",
      " [ 225.64363098  236.20195007  231.77651978 ...,  236.02098083\n",
      "   238.90765381  228.1285553 ]]\n"
     ]
    }
   ],
   "source": [
    "gpu_result = device_matC.copy_to_host()\n",
    "print(gpu_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for timing function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def time_took(functor):\n",
    "    ts = timer()\n",
    "    functor()\n",
    "    te = timer()\n",
    "    return te - ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for that uses ``gpu_matrix_mult()`` with manual memory transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gpu_manual_memory(matA, matB):\n",
    "    device_matC = cuda.device_array_like(matA)\n",
    "    device_matA = cuda.to_device(matA)\n",
    "    device_matB = cuda.to_device(matB)\n",
    "    gpu_matrix_mult[griddim, blockdim](device_matA, device_matB, device_matC)\n",
    "    device_matC.copy_to_host()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate timing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   numba cpu matrix mult: 5.59 seconds\n",
      "   numba gpu matrix mult (auto transfer): 1.15 seconds\n",
      " numba gpu matrix mult (manual transfer): 1.01 seconds\n"
     ]
    }
   ],
   "source": [
    "res = np.empty_like(matA)\n",
    "cpu_time = time_took(lambda: cpu_matrix_mult(matA, matB, res))\n",
    "gpu1_time = time_took(lambda: gpu_matrix_mult[griddim, blockdim](matA, matB,\n",
    "                                                                 res))\n",
    "gpu2_time = time_took(lambda: gpu_manual_memory(matA, matB))\n",
    "\n",
    "assert gpu2_time < gpu1_time < cpu_time\n",
    "\n",
    "fmt = \"{0:>40s}: {1:.2f} seconds\"\n",
    "print(fmt.format(\"numba cpu matrix mult\", cpu_time))\n",
    "print(fmt.format(\"numba gpu matrix mult (auto transfer)\", gpu1_time))\n",
    "print(fmt.format(\"numba gpu matrix mult (manual transfer)\", gpu2_time))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 0
}